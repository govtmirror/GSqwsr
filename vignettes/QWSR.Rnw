%\VignetteIndexEntry{Introduction to the GSqwsr package}
%\VignetteEngine{knitr::knitr}
%\VignetteDepends{}
%\VignetteSuggests{xtable}
%\VignettePackage{GSqwsr}

\documentclass[a4paper,11pt]{article}

\usepackage{amsmath}
\usepackage{times}
\usepackage{hyperref}
\usepackage[numbers, round]{natbib}
\usepackage[american]{babel}
\usepackage{authblk}
\usepackage{subfig}
\usepackage{placeins}
\usepackage{footnote}
\usepackage{tabularx}
\renewcommand\Affilfont{\itshape\small}

\renewcommand{\topfraction}{0.85}
\renewcommand{\textfraction}{0.1}
\usepackage{graphicx}


\textwidth=6.2in
\textheight=8.5in
\parskip=.3cm
\oddsidemargin=.1in
\evensidemargin=.1in
\headheight=-.3in

%------------------------------------------------------------
% newcommand
%------------------------------------------------------------
\newcommand{\scscst}{\scriptscriptstyle}
\newcommand{\scst}{\scriptstyle}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Rclass}[1]{\textit{#1}}
\newcommand{\Rpackage}[1]{\textit{#1}}
\newcommand{\Rexpression}[1]{\texttt{#1}}
\newcommand{\Rmethod}[1]{{\texttt{#1}}}
\newcommand{\Rfunarg}[1]{{\texttt{#1}}}

\begin{document}


<<openLibrary, echo=FALSE>>=
library(xtable)
options(continue=" ")
options(width=60)
library(knitr)

@


%------------------------------------------------------------
\title{The GSqwsr R package}
%------------------------------------------------------------
\author[1]{Laura De Cicco}
\author[1]{Steve Corsi}
\author[1]{Austin Baldwin}
\affil[1]{United States Geological Survey}


<<include=TRUE ,echo=FALSE,eval=TRUE>>=
opts_chunk$set(highlight=TRUE, tidy=TRUE, keep.space=TRUE, keep.blank.space=FALSE, keep.comment=TRUE, tidy=FALSE,comment="")
knit_hooks$set(inline = function(x) {
   if (is.numeric(x)) round(x, 3)})
knit_hooks$set(crop = hook_pdfcrop)

@


\maketitle
\tableofcontents

%------------------------------------------------------------
\section{Introduction to GSqwsr package}
%------------------------------------------------------------ 
The GSqwsr (USGS water quality surrogate regressions) package was designed to simplify the process of gathering water quality sample data and unit surrogate data, running a stepwise regression using the USGSwsQW censReg regression function, and analyzing those results. This vignette will first show a general overview workflow  (\ref{sec:workflow}), then a more detailed description of the workflow with working examples (\ref{sec:details}).

%------------------------------------------------------------
\section{General Workflow}
\label{sec:workflow}
%------------------------------------------------------------ 

<<start,eval = FALSE>>=
library("GSqwsr")

#Sample data included with package:
DTComplete <- StLouisDT
UV <- StLouisUV
QWcodes <- StLouisQWcodes
siteINFO <- StLouisInfo

investigateResponse <- "Ammonia.N"
transformResponse <- "lognormal"

DT <- DTComplete[c(investigateResponse,
                   getPredictVariables(names(UV)), 
                   "decYear","sinDY","cosDY","datetime")]
DT <- na.omit(DT)

predictVariables <- names(DT)[-which(names(DT) 
                  %in% c(investigateResponse,"datetime","decYear"))]


#Check predictor variables
predictVariableScatterPlots(DT,investigateResponse)

# Create 'kitchen sink' formula:
kitchenSink <- createFullFormula(DT,investigateResponse)

#Run stepwise regression with "kitchen sink" as upper bound:
returnPrelim <- prelimModelDev(DT,investigateResponse,kitchenSink,
                               "BIC", #Other option is "AIC"
                               transformResponse)

steps <- returnPrelim$steps
modelResult <- returnPrelim$modelInformation
modelReturn <- returnPrelim$DT.mod

# Analyze steps found:
plotSteps(steps,DT,transformResponse)
analyzeSteps(steps, investigateResponse,siteINFO)

# Analyze model produced from stepwise regression:
resultPlots(DT,modelReturn,siteINFO)
resultResidPlots(DT,modelReturn,siteINFO)

# Create prediction plots
predictionPlot(UV,DT,modelReturn,siteINFO=siteINFO)

@


%------------------------------------------------------------
\section{Workflow Details}
\label{sec:details}
%------------------------------------------------------------
In this section, we will step through the basic workflow.

%------------------------------------------------------------
\subsection{Data Retrieval}
%------------------------------------------------------------
Data retrieval is currently supported by web service calls to the National Water Information Service (NWIS). The first step is to get the discrete sample data that the regressions are modeling. In this example, we will look at the St Louis River at Scanlon (USGS site ID 04024000). If we don't know the sample data that is available, we can use the whatQW function to discover that information. 

<<openGSqwsr,echo=TRUE,eval=FALSE>>=
library(GSqwsr)
@

<<openGSqwsrHidden,echo=FALSE,eval=TRUE, message=FALSE>>=
library(GSqwsr)
@


<<whatQW,echo=TRUE,eval=TRUE>>=

site <- "04024000"  
QWcodes <- whatQW(site, minCount=20)
head(QWcodes)
@

Most likely, there will be a known set of parameters that are to be modeled. If the parameter codes for these analytes are known, the data from NWIS can be accessed directly with the function importNWISqw. The following example shows the process, and then lists the column names returned in the QW dataframe.


<<importNWISqw,echo=TRUE,eval=FALSE>>=
pCodeQW <- c("00608","00613","00618")
startDate <- "2011-04-22"
endDate <- ""
QW <- importNWISqw(site, params=pCodeQW, 
                   begin.date=startDate, end.date=endDate)
@

<<importNWISqwHidden,echo=FALSE,eval=TRUE>>=
pCodeQW <- c("00608","00613","00618")
startDate <- "2011-04-22"
endDate <- ""
QW <- StLouisQW
@

<<qwColNames,echo=TRUE,eval=TRUE>>=
names(QW)
@


This brings the data in automatically as a `qw' object. This means that censoring information is embedded within each data point. If any processing needs to be done to the data, it might be easier to import the raw data first, then convert to 'qw' objects with the makeQWObjects function.

<<makeQWObjects,echo=TRUE,eval=FALSE>>=

QWRaw <- retrieveNWISqwData(site,pCodeQW,startDate,
                            endDate,expanded=TRUE)
QW <- makeQWObjects(QWRaw)
@

Next, the unit value data that will be used as surrogates for the analytes should be retrieved. If the parameters are not known, they can be discovered using the getDataAvailability function, filtering just the `uv' (unit value) data:

<<getDataAvailability,echo=TRUE,eval=TRUE>>=
UVcodes <- getDataAvailability(site)
UVcodes <- UVcodes[UVcodes$service == "uv",]
names(UVcodes)
UVcodes$parameter_cd
@

Finally, the unit value data can be retrieved with the getMultipleUV function. Because of the potentially large amount of data being returned, the web service call is automatically split into individual parameter codes.

<<getMultipleUV,echo=TRUE,eval=FALSE>>=
UVpCodes <- c("00010","00060","00095","00300","00400","63680")
UV <- getMultipleUV(site, startDate, endDate, UVpCodes)

@

<<getMultipleUVHidden,echo=FALSE,eval=TRUE>>=
UVpCodes <- c("00010","00060","00095","00300","00400","63680")
UV <- StLouisUV
@

<<uvColNames,echo=TRUE,eval=TRUE>>=
names(UV)
@


%------------------------------------------------------------
\subsection{Data Merging}
%------------------------------------------------------------
We now need to merge the sample and continuous data into one dataframe. This is accomplished using the mergeDatasets function. Both QW and UV dataframes need a column called `datetime' that has the date and time in an POSIXct object. This may need to be done as shown below.


<<mergeDatasets,echo=TRUE,eval=TRUE>>=
QW$datetime <- as.POSIXct(paste(QW$sample_dt," ",QW$sample_tm, ":00",sep=""))

# Make sure they are in consistant time zones:
QW$datetime <- setTZ(QW$datetime, QW$tzone_cd)
UV$datetime <- setTZ(UV$datetime, UV$tz_cd)

mergeReturn <- mergeDatasets(QW, UV, QWcodes)
DTComplete <- mergeReturn$DTComplete
QWcodes <- mergeReturn$QWcodes
@

The dataframe DTComplete contains a column of each of the discrete samples, and a column of the nearest (temporally) unit value data. The function mergeDatasets has an argument called `max.diff'. The default is set to `2 hours', meaning that if the sample and continuous data timestamps do not match, the merge will take the closest continuous data within 2 hours. This value can be changed, see ?mergeNearest for more options.

%------------------------------------------------------------
\subsection{Data Investigation}
%------------------------------------------------------------

%------------------------------------------------------------
\subsubsection{Narrow down investigation}
%------------------------------------------------------------


We now want to narrow our investigation down to one analyte. Let us look at nitrate. First we will want a dataframe DT with just nitrate and the unit values. We will call these the `prediction values' because they will eventually be used to predict nitrate.

<<getDT,echo=TRUE,eval=TRUE>>=
investigateResponse <- "Nitrate.N"
predictionVariables <- getPredictVariables(names(UV))

DT <- DTComplete[c(investigateResponse,
                   predictionVariables, 
                   "decYear","sinDY","cosDY","datetime")]

names(DT)
@

For the regression, there can be no NA values in any of the columns. There are many ways in R to deal with this requirement. The easiest way to do it is remove any row that has any NA. This can be done as follows:

<<rmNADT,echo=TRUE,eval=TRUE>>=
DT <- na.omit(DT)
@

There may be other situations where you want to remove a column that contains the majority of the missing data.

%------------------------------------------------------------
\subsubsection{Plot variables}
%------------------------------------------------------------
There are a few tools included in this package to explore the data before performing the regression.

<<plotQQTransforms,echo=TRUE,eval=TRUE,fig.cap="plotQQTransforms">>=
plotQQTransforms(DT,investigateResponse)
@


<<predictVariableScatterPlots,echo=TRUE,eval=TRUE,fig.cap="predictVariableScatterPlots", message=FALSE>>=
predictVariableScatterPlots(DT,investigateResponse)
@

\FloatBarrier
%------------------------------------------------------------
\subsection{Stepwise Regression}
%------------------------------------------------------------
We are ready to perform a stepwise regression of the data to find the most significant variables to use in the model. This is accomplished with the prelimModelDev function. There are several inputs to this function. DT is the dataframe with all the predictor variables as well as the response we are investigating. We also need to define an upper bound for the stepwise regression to test. This is an equation with all the possible predictor variables, along with their transforms that we are interested in testing. If we want to use all possible variables, and all available transforms, we can use the equation createFullFormula (continuing with our Chloride example):

<<createFullFormula,echo=TRUE,eval=TRUE,echo=TRUE>>=
upperBoundFormula <- createFullFormula(DT,investigateResponse)
@

<<createFullFormula2,echo=TRUE,eval=TRUE,echo=FALSE,results='markup'>>=
substring(upperBoundFormula, first=0,last=59)
substring(upperBoundFormula, first=60,last=119)
@

The function will check if any data in DT has less than or equal to zero values. If so, a log transform is not included.

Now to use the stepwise regression within the prelimModelDev function. In this function, the DT dataframe is required, the column name of the response variable (in this example, investigateResponse), and the upper bound formula. The user can then choose a value for k which can be `AIC' (akaike information criterion), `BIC' (Bayesian information criterion), or a value of the multiple of the number of degrees of freedom used for the penalty. BIC has a harsher penalty for overfitting the model, which is typically seen as a benefit. The default is BIC. Finally, transformResponse can either be `lognormal' or `normal', which will define the transformation of the response variable. 

Additionally, this function has an argument 'autoSinCos' which is a logical input. The default is set to TRUE, in this case - if the sine of decimal year (sinDY) is picked during the stepwise regression, the next step is forced to be cosDY. Likewise, if cosDY is picked, sinDY is forced on the next step. This feature can be turned off by setting autoSinCos to FALSE.


<<prelimModelDev,echo=TRUE,eval=TRUE,echo=TRUE>>=
transformResponse <- "lognormal"

returnPrelim <- prelimModelDev(DT,
                 investigateResponse,
                 upperBoundFormula,
                 "BIC", #Other option is "AIC"
                 transformResponse)

steps <- returnPrelim$steps
modelResult <- returnPrelim$modelInformation
modelReturn <- returnPrelim$DT.mod
@

The output during the function shows the steps that the stepwise regression determined were ideal, information from the final model (modelInformation) such as the terms, their coefficients, standard error, p value as calculated by the censReg function, and standard coefficient (PARAML/STDDEV), and the raw data returned from censReg (DT.mod). 

\FloatBarrier

%------------------------------------------------------------
\subsection{Stepwise Regression Analysis}
%------------------------------------------------------------
It might be a good idea here to verify that the results from the stepwise regression are indeed what you want. The process can be observed using two functions: plotSteps and analyzeSteps.

\FloatBarrier
analyzeSteps creates a plot with correlation, slope, RMSE, PRESS, and AIC statistics. Correlation and slope are values that should trend towards 1. Correlation is the correlation between observed and predicted, slope is the slope of observed and predicted. RMSE should trend towards zero, RMSE is the root mean squared error of the observed vs. predicted data. PRESS (predicted residual sums of squares) is calculated from the external studentized residuals, and should trend downward (for a better model fit). Residuals are calculated for censored values using the detection limit. AIC (akaike information criterion) is returned from the ANOVA output of the stepwise regression. It is always called `AIC' whether or not `AIC' or `BIC' was specified. AIC will also trend downward for better model fits.


<<analyzeSteps,echo=TRUE,eval=TRUE,echo=TRUE,fig.cap="analyzeSteps">>=
siteINFO <- getSiteFileData(site, interactive=FALSE)
analyzeSteps(steps, investigateResponse,siteINFO,
             xCorner=0.01,yCorner=0.3)
@

In this case, it may seem strange that the AIC value goes up then down. This is because the first parameter that was picked was sinDY (sine of decimal year). As mentioned earlier, if sinDY is picked, we automatically force cosine to be the next parameter. 

\FloatBarrier

plotSteps shows the observed versus predicted for each step along the way of the stepwise regression.There are two lines included, the blue line is a one-to-one line, the red line is the slope of the observed versus predicted values as calculated with a linear regression (lm). In this simple regression, censored values are taken as their detection limits. Red points indicate potential outliers as calculated based on external studentized residuals greater than 3 or less than -3. Censored values are represented with a line segment from the detection limit to zero (for left-censored data).


<<plotSteps,echo=TRUE,eval=TRUE,fig.cap="plotSteps">>=

m <- t(matrix(c(1:6), nrow = 2, ncol = 3))
layout(m)
par(mar=c(2,2,2,2))
plotSteps(steps,DT,transformResponse)

@

\FloatBarrier

%------------------------------------------------------------
\subsection{Model Adjustments}
%------------------------------------------------------------
There may be situations in which the user wants to explore alternative models compared to the results of the stepwise regression. The first tool offered in the package for this type of work is the function generateParamChoices. This function will create a dataframe, and optionally save it to a csv file with all of the parameter choices.

<<generateParamChoices,echo=TRUE,eval=FALSE>>=
#Change this to a relavent path:
pathToSave <- "C:/RData/"
choices <- generateParamChoices(predictionVariables,
                                modelReturn,pathToSave,save=TRUE)

@

<<generateParamChoices2,echo=FALSE,eval=TRUE>>=
choices <- generateParamChoices(predictionVariables,modelReturn)
@

This produces a file that can be opened in Microsoft Excel, or any text editor:

\begin{figure}[ht!]
\centering
 \resizebox{0.9\textwidth}{!}{\includegraphics{Excel.png}} 
\caption{Output of generateParamChoices shown in Excel}
\label{overflow}
\end{figure}

The first column, `Scalar', is pre-populated with zeros and ones, where ones represent the variables picked in the stepwise regression. Changing the 1's and 0's in this column will allow the user to easily set which parameters should be modeled. So, adding a 1 to the Flow row in the Scalar column will tell the program to include Flow in the model (as well as any other parameters with 1's). The next set of columns are used to allow users to include interaction terms. For example, if the interaction between log(Flow) and Turbidity was thought to be useful, a 1 in row 8, column H (log(Flow):Turb) would be required.

Once the parameter choice file is adjusted, it can be read in using read.csv, and a new formula can be created using the createFormulaFromDF function.

<<createFormulaFromDF,echo=TRUE,eval=FALSE>>=
choicesNew <- read.csv(pathToSave)
newFormula <-createFormulaFromDF(choicesNew)
@

<<createFormulaFromDF2,echo=FALSE,eval=TRUE>>=
choicesNew <- choices
choicesNew[7,8] <- 1
choicesNew[2,2] <- 1
newFormula <-createFormulaFromDF(choicesNew)
@

<<createFormulaFromDF3,echo=TRUE,eval=TRUE>>=
newFormula
@

From this formula, the stepwise regression routine can be re-run (if certain parameters were deleted for example), or the model can be created:

<<censReg, echo=TRUE,eval=TRUE>>=
newUpperFormula <- paste(investigateResponse," ~ ", newFormula, sep="")
modelReturnCustom <- censReg(newUpperFormula, 
                       dist=transformResponse, data=DT)
modelReturnCustom
@

%------------------------------------------------------------
\subsection{Model Analysis}
%------------------------------------------------------------
Finally, the package offers several ways to analyze and report on the model results. We will look at the results of the original model returned from the stepwise regression (not the custom model we created in the last section). Censored values are plotted as points with segments (from the detection limit towards zero for left-censored data). Also, left-censored residuals are calculated using the detection limit.

The function resultPlots plots a set of plots. All model results include observed vs. predicted (A), residuals vs. predicted (B), residuals vs. time (C), and residual quantiles vs. theoretical quantiles (D). After that, there is a plot for observed vs. each parameter in the model (E...).

<<resultPlots,echo=TRUE,eval=TRUE,echo=TRUE,fig.cap="resultPlots">>=
resultPlots(DT,modelReturn,siteINFO)
@

\FloatBarrier

The function resultResidPlots plots a set of plots. All model results include observed vs. predicted (A), residuals vs. predicted (B), residuals vs. time (C), and residual quantiles vs. theoretical quantiles (D). After that, there is a plot for residuals vs. each parameter in the model (E...).


<<resultResidPlots,echo=TRUE,eval=TRUE,echo=TRUE,fig.cap="resultResidPlots">>=
resultResidPlots(DT,modelReturn,siteINFO)
@

\FloatBarrier

The predictionPlot function plots the predicted values based on all of the unit value data available (from the UV dataframe) in blue. Red dots representing the actual measured data are also included. Left-censored points are shown at their detetction limit, with a segment going towards zero.


<<predictionPlot,echo=TRUE,eval=TRUE,echo=TRUE,fig.cap="predictionPlot">>=
predictionPlot(UV,DT,modelReturn,siteINFO=siteINFO)
@

\FloatBarrier
Finally, a summary printout can be obtained with the function summaryPrintout in either the R console or saved to a file.

<<summaryPrintout,echo=TRUE,eval=TRUE,echo=TRUE>>=
summaryPrintout(modelReturn, siteINFO)
@

\clearpage
\appendix

%------------------------------------------------------------ 
\section{Getting Started in R}
\label{sec:appendix1}
%------------------------------------------------------------ 
This section describes the options for downloading and installing the GSqwsr package.

%------------------------------------------------------------
\subsection{New to R?}
%------------------------------------------------------------ 
If you are new to R, you will need to first install the latest version of R, which can be found here: \url{http://www.r-project.org/}.

There are many options for running and editing R code, one nice environment to learn R is RStudio. RStudio can be downloaded here: \url{http://rstudio.org/}. Once R and RStudio are installed, the dataRetrieval package needs to be installed as described in the next section.

At any time, you can get information about any function in R by typing a question mark before the functions name.  This will open a file (in RStudio, in the Help window) that describes the function, the required arguments, and provides working examples.

<<helpFunc,eval = FALSE>>=
library(GSqwsr)
?plotSteps
@

To see the raw code for a particular code, type the name of the function:
<<rawFunc,eval = FALSE>>=
plotSteps
@

%------------------------------------------------------------
\subsection{R User: Installing QWSR}
%------------------------------------------------------------ 
Before installing GSqwsr, the dependent packages must be first be installed:

<<installFromCran,eval = FALSE>>=
install.packages(c("XML", "lubridate", "akima", "KernSmooth",
                   "leaps", "car", "mvtnorm", "digest",
                   "relimp", "BSDA", "RODBC","memoise",
                   "boot","survival","splines","RColorBrewer",
                   "lattice","MASS"), dependencies=TRUE)
install.packages(c("USGSwsBase","USGSwsData","dataRetrieval",
                   "USGSwsGraphs","USGSwsStats",
                   "USGSwsQW","GSqwsr"), 
                 repos="http://usgs-r.github.com")
@


After installing the package, you need to open the library each time you re-start R.  This is done with the simple command:
<<openLibraryTest, eval=FALSE>>=
library(GSqwsr)
@




\end{document}
